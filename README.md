# ibet

---
<a id="ibet-scraping-section"></a>
## ~~~ 1. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ~~~
--- 

–î–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –ø–æ–æ—á–µ—Ä–µ–¥–Ω–æ —Å —Ç—Ä–µ—Ö —Å–∞–π—Ç–æ–≤ –∏ –∏ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ Redis –ª–∏–±–æ Clickhouse.
–í Redis –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –ø–æ Live —Å—Ç–∞–≤–∫–∞–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.

–í Clickhouse –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥—ç—à–±–æ—Ä–¥–∞ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
  
–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤—ã–∫–ª–∞–¥—ã–≤–∞—é —Å–∫—Ä–∏–ø—Ç —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–∞ —Å Parimatch —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –ø—Ä–æ–∫—Ä—É—Ç—É–æ–π. 

–ò–∑ –∏–Ω—Ç–µ—Ä—Å–Ω–æ–≥–æ - –¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ –Ω–∞ –¥–∏—Å–ø–ª–µ–π, –æ–±–æ–π—Ç–∏ —ç—Ç—É –∑–∞—â–∏—Ç—É –Ω–µ–ª—å–∑—è. 
–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ - –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –¥–∏—Å–ø–ª–µ–π + —É–º–µ–Ω—å—à–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞


<details>
  <summary><strong>üñºÔ∏è –í–Ω–µ—à–Ω–∏–π –≤–∏–¥ —Å–∞–π—Ç–∞</strong></summary>

  ![–í–Ω–µ—à–Ω–∏–π –≤–∏–¥ —Å–∞–π—Ç–∞](https://raw.githubusercontent.com/sazhiromru/images/refs/heads/main/pari-live-section.PNG)
</details>

<details>
  <summary><strong>üìú Parimatch</strong></summary>

```python
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.service import Service
import time
import re
from selenium.webdriver.common.by import By
from random import uniform
from datetime import datetime, timedelta
import os
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.firefox.options import Options
from selenium import webdriver
import clickhouse_connect
import pandas as pd

os.environ["DISPLAY"] = ":1.0"

def initialize_driver():

    profile_path = '/home/venediktovga/.mozilla/firefox/a6k6fc50.default-release'
    options = Options()
    options.set_preference("profile", profile_path)
    

    options.set_preference("layout.css.devPixelsPerPx", "0.25")  # –£–º–µ–Ω—å—à–∞–µ–º –ú–∞—Å—à—Ç–∞–± –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    options.set_preference("browser.sessionstore.privacy_level", 2)

    options.add_argument("--disable-infobars")  
    options.add_argument("--start-maximized")
    options.add_argument("--no-sandbox") 
    options.add_argument("--disable-dev-shm-usage")  
    options.add_argument("--start-maximized")
    options.set_preference("permissions.default.image", 2) 
    options.set_preference("browser.cache.disk.enable", False)
    options.set_preference("browser.cache.memory.enable", False)
    options.set_preference("browser.sessionstore.privacy_level", 2)

    service = Service("/usr/local/bin/geckodriver")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ú–æ–∑–∏–ª–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ + –ø–ª–∞–≥–∏–Ω—ã –¥–ª—è user agent
    service.log_path = os.devnull


    driver = webdriver.Firefox(service=service, options=options)
    driver.set_window_size(1920, 1080) 
    print('–ó–∞–ø—É—Å—Ç–∏–ª–∏ –¥—Ä–∞–π–≤–µ—Ä Firefox')
    driver.set_page_load_timeout(70)  
    time.sleep(2) 
    driver.maximize_window()  

    return driver

#–∞—Ä—Ö–∏–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞ –≤—á–µ—Ä–∞—à–Ω–∏–π –¥–µ–Ω—å –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

def source_page(driver):
    date = (datetime.now() - timedelta(days = 1))
    url_part = date.strftime('%Y-%m-%d')
    url = f'https://pari.ru/results?date={url_part}'
    driver.get(url)
    time.sleep(14)
    return date

def extract(driver,date):
    data = driver.page_source
    soup = BeautifulSoup(data, 'html.parser')
    matches = soup.find_all('div', class_='results-event--Me6XJ')
    results_list = []
    for match in matches:
        row = []
        #–¥–∞—Ç–∞
        row.append(date)
        #–∫–∞—Ç–µ–≥–æ—Ä–∏—è
        category = match.find_previous('div', class_ = re.compile(r'results-sport__caption-container--e43SF'))
        row.append(category.text)
        #–ø–æ–¥—Ä–∞–∑–¥–µ–ª
        try:
            subcategory =  match.find_previous('div', class_ = re.compile(r'overflowed-text--JHSWr results-competition__caption--zmv7q'))
            row.append(subcategory.text)
        except Exception:
            row.append('no subcategory')
        teams = match.find_all('div', class_=re.compile(r'results-event-team__name'))
        team_list = [team.get_text(strip = True) for team in teams]
        #–∫–æ–º–∞–Ω–¥–∞ 1 –∏ 2
        row.extend(team_list)
        #—Å–æ–±—ã—Ç–∏–µ
        try:
            row.append(' ‚Äî '.join([team_list[0],team_list[1]]))
        except Exception:
            row.append('-')
        #—Å—á–µ—Ç   
        try:
            scores = match.find_all('div', class_=re.compile(r'results-scoreBlock__score--XvlMM _summary--Jt8Ej _bold--JaGTY'))
            score_list = [str(score.get_text(strip = True)) for score in scores]
            if len(score_list) == 2:
                row.extend(score_list)
            else:
                row.append('-')
                row.append('-')
        except Exception:
            row.append('-')
            row.append('-')
        #–∏—Å—Ö–æ–¥
        try:
            if int(score_list[0])> int(score_list[1]):
                row.append('1')
            elif int(score_list[1])> int(score_list[0]):
                row.append('2')
            elif int(score_list[0]) == int(score_list[1]):
                row.append('X')
            else:
                row.append('-')
        except Exception:
            row.append('-')
        try:
            row.append(int(score_list[0]) - int(score_list[1]))
            row.append(int(score_list[1]) - int(score_list[0]))
        except Exception:
            row.append(-1000)
            row.append(-1000)
        try:
            row.append((int(score_list[0])+int(score_list[1])))
        except Exception:
            row.append(-1000)
        results_list.append(row)
    column_names = ["date", "category", "subcategory", "team1", "team2", "event", "score1", "score2", "stavka", "f1", "f2", "total"]
    df = pd.DataFrame(results_list, columns = column_names)
    df.to_csv('results.csv', index = False, encoding='UTF-8-sig')
    upload(results_list)

def scroll_container(driver,date):
    count_repeat_break = 0
    count = 0
    while True:
        extract(driver,date)
        print(f'–¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω–∞ {count} –∑–∞–≥—Ä—É–∂–µ–Ω—ã')
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏–µ –ø—Ä–æ–º–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ —Ç–æ –∂–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        elements = driver.find_elements(By.CLASS_NAME, "results-event--Me6XJ")
        last_element = elements[-1] if elements else None  
        if last_element:
            driver.execute_script("arguments[0].scrollIntoView({block: 'start', inline: 'nearest'});", last_element)
            count+=1
        print(f'—Å–ø—É—Å–∫ –Ω–æ–º–µ—Ä {count}')
        time.sleep(uniform(9,11))
        elements = driver.find_elements(By.CLASS_NAME, "results-event--Me6XJ")
        new_last_element = elements[-1]
        if new_last_element == last_element:
            count_repeat_break+=1
        else:
            count_repeat_break = 0
        if count_repeat_break == 3:
            break
    client = clickhouse_connect.get_client(host='10.140.0.7', port=8123, username='default', password='Qwer3asdf')
    client.insert('ibet.results_date', [(datetime.now(),)], column_names='date')

#–ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é –≤ CLickhouse

def upload(extracted_data):
    client = clickhouse_connect.get_client(host='10.140.0.7', port=8123, username='default', password='Qwer3asdf')
    client.insert('ibet.results',extracted_data, column_names = ["date", "category", "subcategory", "team1", "team2", "event", "score1", "score2", "stavka", "f1", "f2", "total"]
)

def main():
    try:
        driver = initialize_driver()
        date = source_page(driver)
        scroll_container(driver,date)

    finally:
        driver.quit()

if __name__ == '__main__':
    main()
```

</details>
<br></br>

---
<a id="wrangling-section"></a>
## ~~~ 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ~~~
--- 


### Merge  

–í–Ω–µ—à–Ω–∏–º —Å–ª–∏—è–Ω–∏–µ–º —Å–æ–µ–¥–∏–Ω—è–µ–º —Ç—Ä–∏ csv, –æ–∫—Ä—É–≥—è–ª–µ–º —Ü–∏—Ñ—Ä—ã, –ø—Ä–∏–≤–æ–¥–∏–º –≤–∞–ª—é—Ç—É –∫ –¥–æ–ª–ª–∞—Ä—É, —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—à–∏–±–∫–∏ –∏ –Ω–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ –≤—ã–≥–æ–¥–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –ø–æ –ø—Ä–æ–¥–∞–∂–µ –∫–∏—Ç–∞–π->—Ä—Ñ –∏ —Ä—Ñ->–∫–∏—Ç–∞–π
<details>
  <summary><strong>üìú Merge </strong></summary>

```python
import pandas as pd
import numpy as np
from datetime import datetime

timestamp = datetime.now().strftime('%d-%m-%Y')

path_c5 = f'c5game_{timestamp}.csv'
path_market = f'market_{timestamp}.csv'
path_buff = f'buff_{timestamp}.csv'
path_buff_buyorders = f'buff_buyorders_{timestamp}.csv'

#–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—É—Ä—Å–∞ –Ω–µ—Ç, –Ω–æ –∫—É—Ä—Å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —é–∞–Ω—å/–¥–æ–ª–ª–∞—Ä –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è –º–Ω–æ–≥–æ –ª–µ—Ç. 
cny_usd = 0.14
profit_coef = 0.9025

df_c5 = pd.read_csv(path_c5, encoding = 'utf-16')
df_c5.drop_duplicates()
df_c5.rename(columns = {'c5_item':'Item'},inplace = True)
#–°–æ–∑–¥–∞–Ω —Ñ—Ä–µ–π–º —Å5 —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'Item', 'c5_price'

df_buff_buyorders = pd.read_csv(path_buff_buyorders, encoding = 'utf-16')
df_buff_buyorders.rename(columns = {'buff_item':'Item', 'buff_price':'price'}, inplace = True)
#–°–æ–∑–¥–∞–Ω —Ñ—Ä–µ–π–º buff_buyorders —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'Item', 'buyorders_price'

df_market = pd.read_csv(path_market, encoding = 'utf-16')
#–°–æ–∑–¥–∞–Ω —Ñ—Ä–µ–π–º market —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'Item', 'market_price'

df_buff = pd.read_csv(path_buff, encoding = 'utf-16')
df_buff.rename(columns = {'buff_item':'Item'},inplace = True )
#–°–æ–∑–¥–∞–Ω —Ñ—Ä–µ–π–º buff —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'Item', 'buff_price'

df_final = pd.merge(df_buff, df_market, on = 'Item', how = 'outer')
df_final = pd.merge(df_final, df_c5, on = 'Item', how = 'outer')
# –≤–Ω–µ—à–Ω–∏–π –º–µ—Ä–¥–∂

df_final['c5_price'] =  df_final['c5_price'].astype(float).fillna(0)
df_final['buff_price'] =  df_final['buff_price'].astype(float).fillna(0)
df_final['market_price'] =  df_final['market_price'].astype(float).fillna(0)

df_final['buff_price'] = df_final['buff_price'].astype(float).apply(lambda x : x*cny_usd)
df_final['c5_price'] = df_final['c5_price'].astype(float).apply(lambda x : x*cny_usd)
# –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞, –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –π–µ–Ω –∫ –¥–æ–ª–ª–∞—Ä–∞–º

'''–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –ø–æ –ø—Ä–æ–¥–∞–∂–µ –∫–∏—Ç–∞–π -> –º–∞—Ä–∫–µ—Ç, –≤—ã–±–æ—Ä –ª—É—á—à–∏—Ö —Å–¥–µ–ª–æ–∫ –Ω–∞ c5game and buff163'''
df_direct = df_final.copy(deep = True)

df_direct['market_price'] = df_direct['market_price'].astype(float).apply(lambda x: x*profit_coef)
df_direct['market_c5'] = (df_direct['market_price']/df_direct['c5_price'])
df_direct['market_buff'] = (df_direct['market_price']/df_direct['buff_price'])
df_direct.replace([np.inf, -np.inf], 0, inplace=True)

df_direct['best_price'] = np.maximum(df_direct['market_buff'], df_direct['market_c5'])
df_direct['label'] = np.where(df_direct['market_buff']>df_direct['market_c5'],'buff','c5')
df_direct = df_direct.sort_values(by = 'best_price', ascending=False)
#

columns_to_round = ['c5_price', 'market_price', 'buff_price', 'market_c5', 'market_buff', 'best_price']
df_direct[columns_to_round] = df_direct[columns_to_round].round(4)
df_direct['Item'] = df_direct['Item'].str.strip()
df_direct.drop_duplicates(subset=['Item'], inplace=True)
df_direct.reset_index(drop=True, inplace=True)

df_direct.drop(columns=['Date','Date_x','Date_y'], inplace=True)
df_direct['Date'] = timestamp

df_direct.drop_duplicates(inplace=True)

df_direct.to_csv(f'direct_{timestamp}.csv',index = False, encoding = 'utf-16')

'''–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –ø–æ –ø—Ä–æ–¥–∞–∂–µ –º–∞—Ä–∫–µ—Ç -> –∫–∏—Ç–∞–π, –≤—ã–±–æ—Ä –ª—É—á—à–∏—Ö —Å–¥–µ–ª–æ–∫ –ø–æ –æ—Ä–¥–µ—Ä–∞–º –Ω–∞ –ø–æ–∫—É–ø–∫—É –Ω–∞ –±–∞—Ñ—Ñ'''
df_reverse = pd.merge(df_market, df_buff_buyorders, on = 'Item', how = 'outer')
df_reverse = df_reverse.fillna(0)
df_reverse['buyorders_price'] = df_reverse['buyorders_price'].astype(float).apply(lambda x: x*cny_usd)
df_reverse['coef'] = df_reverse['buyorders_price'] / df_reverse['market_price']
df_reverse.replace(np.inf,0, inplace = True)
df_reverse.sort_values(by = 'coef', ascending=False, inplace = True)

df_reverse.drop(columns=['Date_x','Date_x'], inplace=True)
df_reverse['Date'] = timestamp
df_reverse.drop_duplicates(inplace = True)

df_reverse.to_csv(f'reverse_{timestamp}.csv',index = False, encoding = 'utf-16')
```
</details>  
<br>
